# Artalytics R Packages ‚Äì AGENTS Guidelines

This document defines the shared expectations and tooling standards for all R packages in the Artalytics platform. It serves as a common reference for **agentic tools** (e.g., GitHub Copilot Agents, Sourcegraph, Cursor) to interact with each repository consistently and with full context. It covers both the **internal utility libraries** (like `artcore`, `artutils`, `artopenai`, etc.) and the **Shiny module packages** (like `modBrowse`, `modUpload`, `modGallery`, etc.) that together make up the Artalytics ecosystem.

## Project Purpose and Ecosystem Overview

The Artalytics platform is composed of multiple R packages, each serving a specific role but adhering to common standards:

* **Utility Libraries (Internal):** Packages such as **`artcore`** and **`artutils`** provide foundational functions and shared utilities for the platform. These are not directly user-facing but are consumed by other packages. For example, `artcore` contains core platform functions, while `artutils` builds on `artcore` to offer tools and data shared across modules.
* **Integration Libraries:** Additional internal packages like **`artopenai`** (OpenAI integration) or **`pixelsense`** (image processing) extend functionality. They follow the same standards, ensuring they integrate seamlessly with core utilities.
* **UI Module Packages (Shiny-facing):** Packages such as **`modBrowse`**, **`modUpload`**, and **`modGallery`** implement Shiny modules (UI components + server logic) for the platform‚Äôs web interface. These modules are meant to be plugged into the main Artalytics Shiny application. They are developed as independent packages for modularity, but share the platform‚Äôs coding conventions and infrastructure.

All these packages are versioned, documented, and tested consistently. This AGENTS.md governs the **entire ecosystem**, ensuring that whether a package is backend-focused or UI-facing, it conforms to the same high-level guidelines. The goal is to make it easy for developers and AI agents to navigate and contribute to any Artalytics repository knowing that a common structure and philosophy applies.

## Directory Structure

All Artalytics R packages use the **standard R package layout** with some additions for Shiny modules:

* **Root Package Structure:** Each repository is an R package with the conventional directories:

  * `R/` ‚Äì Core R source code (functions, module definitions, etc.).
  * `man/` ‚Äì Documentation files (`.Rd` help files) generated by Roxygen2.
  * `tests/` ‚Äì Unit tests (using **testthat**) in `tests/testthat/` subdirectory.
  * `inst/` ‚Äì Additional files to install (e.g., `inst/extdata` for example data, or `inst/www` for Shiny assets). Shiny module packages may put UI asset files (HTML, JavaScript, CSS) or example apps here.
  * `vignettes/` ‚Äì Long-form documentation or tutorials (often written in R Markdown or Quarto) for the package.
  * `.github/workflows/` ‚Äì Continuous Integration (CI) workflows (YAML files for GitHub Actions) for build, test, lint, etc.
* **Quarto/Shiny Components:** In Shiny module packages, you may find Quarto or Shiny app components included. For example, a module package might include a Quarto document in `vignettes/` demonstrating usage, or a small Shiny example app under `inst/` to manually test the module‚Äôs UI. Static assets for the module‚Äôs UI (images, JS, CSS) are stored under `inst/` (often `inst/www`) so that they are deployed with the package.
* **Project Files:** Each repo has a `.Rproj` file configured for package development. This ensures, for instance, that building the package will run Roxygen2 and include all necessary documentation and metadata.

Every package‚Äôs directory structure is thus predictable, which helps automation tools and contributors quickly locate code, tests, and docs. The consistency in layout means agents or developers can transfer knowledge of one package‚Äôs structure to all others.

## Build and Documentation

All Artalytics packages share the same build process and documentation standards to ensure uniformity:

* **RStudio Build Tools:** Packages are developed with RStudio, using the option **‚ÄúGenerate documentation with Roxygen‚Äù** enabled. This means that documentation is written as Roxygen2 comments in the source code, and building the package will automatically produce up-to-date help files. The RStudio project settings (in the `.Rproj` file) specify that on build, Roxygen should update **`.Rd` files, the Collate order, the `NAMESPACE`, and vignettes**. In short, code and documentation are kept in sync.
* **Roxygen2 Documentation:** All exported functions and user-facing objects **must have Roxygen2 doc comments**. These comments produce the manual pages in `man/`. The documentation covers usage, arguments, examples, and references as needed. This ensures agentic tools and developers can query any function for help and get clear information.
* **Collate and NAMESPACE:** The build process updates the **Collate** field (the order in which R source files are loaded) and the **`NAMESPACE`** file (which controls exported functions and imports) via Roxygen directives. Developers should not manually edit `NAMESPACE` ‚Äì exports and imports are managed by Roxygen (using tags like `@export`, `@import`, etc.).
* **Vignettes:** Packages can include vignettes (in `vignettes/`), written in R Markdown or Quarto, to provide tutorials or extensive examples. These are built during package build (often as part of `R CMD build`) so that they are available to users of the package. Each module package might have a vignette demonstrating how to use it within the Artalytics app.
* **Source Preservation:** Package installations and checks are done with source references kept. In practice, packages are built with the `--with-keep.source` flag enabled so that the R bytecode retains line numbers and source references. This is crucial for debugging (e.g., error stack traces will show the source file/line). The RStudio project is configured to include this flag on install.
* **R CMD Check:** Every package must pass **`R CMD check`** without errors or warnings. This means all examples, tests, and documentation are run/tested during the build. The CI pipelines automatically run R CMD check on multiple platforms to catch issues early. Developers are expected to run R CMD check locally before pushing changes. This ensures that all packages remain CRAN-quality in terms of documentation and build integrity.

By adhering to these build and documentation standards, we guarantee that all packages have consistent, high-quality documentation and are built reproducibly. Agent tools can rely on Roxygen comments to provide in-context help, and contributors can expect a similar development experience across packages.

## Testing Standards

Robust testing is a cornerstone of the Artalytics packages. All packages follow these testing practices:

* **testthat Framework:** We use **testthat** (edition 3) for unit testing across all packages. Every package has a `tests/testthat/` directory with test files, and each package‚Äôs DESCRIPTION marks testthat as a suggested dependency (often with `Config/testthat/edition: 3` set for the latest features). This uniform use of testthat means that running `devtools::test()` or `R CMD check` will execute the package‚Äôs tests automatically.
* **Coverage of Core Logic:** Tests are written to cover all critical logic paths, edge cases, and error conditions. Both utility packages and module packages include tests for their functions (including server-side module logic). UI modules are structured so that as much logic as possible can be exercised in a non-interactive way (e.g., testing a module‚Äôs reactive outputs with simulated inputs).
* **Data Table Expectations:** Where functions produce tabular data (e.g., return a `data.table` or `data.frame`), tests utilize **data.table-based expectations**. Rather than converting outputs to tibbles or other forms, tests directly compare using data.table where appropriate. For example, if a function returns a `data.table`, tests might use `identical()` or `all.equal()` on sorted tables, or check specific columns and counts using data.table syntax. This ensures we test the actual structures we use in production without coercing them (preserving type and key information).
* **Snapshot and UI Testing:** For Shiny module packages, testing includes UI considerations:

  * We may use **snapshot tests** (via `expect_snapshot()` from testthat) to capture the output of functions or printed output for stability across changes.
  * For interactive Shiny outputs, **shinytest2** (the modern Shiny testing framework) may be employed. Shinytest2 can simulate a Shiny session and verify that the module behaves as expected (e.g., inputs lead to correct outputs or UI state). These tests help ensure that the modules will integrate correctly into the live application.
* **Testing Data and Environment:** If tests require sample data or specific environment variables (e.g., `ART_USE_PG_CONF` or demo mode flags), these are set up within the tests. We avoid tests that depend on external systems or secrets. For instance, tests that require an OpenAI API call are either mocked or skipped in CI (to avoid needing the actual key). The CI environment variables (like `ART_USE_PG_CONF=artprod` or `ART_OPENAI_KEY=NO_VALUE`) help tests run in a consistent mode on the CI server.

All new features or bug fixes should come with corresponding tests. The guiding principle is **if it‚Äôs important to the package‚Äôs functionality, it should be covered by tests**. This not only prevents regressions but also provides examples of expected behavior (useful for both developers and AI agents trying to understand the code).

## Style Guide and Coding Conventions

We maintain a consistent coding style across all R packages to make the codebase easy to read and navigate. Key style guidelines include:

* **Favor `data.table` over tidyverse:** We prefer using `data.table` for data manipulation rather than `dplyr` or other tidyverse packages. This means using data.table‚Äôs syntax (`DT[i, j, by]` and chaining with `[]`) for subsetting and aggregation. The codebase avoids attaching the tidyverse; instead it leans on base R and data.table for performance and clarity. This also reduces external dependencies.
* **Native Pipe Operator:** All packages use R‚Äôs native pipe `|>` (introduced in R 4.1+) instead of the magrittr `%>%`. The native pipe is enabled in the project settings (e.g., `UseNativePipeOperator: Yes` in the RProj file). Using `|>` ensures consistency and avoids an extra dependency. Code should be written in a pipe-friendly style when it improves readability (especially for sequential transformations).
* **String Handling:** We prefer **stringr** (from the stringr/stringi package) for string operations over base R string functions. For example, use `stringr::str_detect`, `str_replace`, etc., instead of base R‚Äôs `grepl`, `gsub`, etc. This provides more readable and robust regex handling. All string constants should be written clearly, and where patterns are complex, commented for clarity.
* **Concatenation:** Use `paste0()` instead of `paste()` when concatenating strings without a separator. In general, be mindful of unnecessary whitespace or separators in output. Consistency in constructing strings (especially for file paths or messages) makes the behavior predictable.
* **Code Conciseness:** Aim to write code that is **concise yet readable**. We favor a smaller footprint in terms of lines of code ‚Äì for instance, using vectorized operations or data.table chaining ‚Äì rather than overly verbose iterative code. However, never sacrifice clarity: if a single-line expression is too complex, it‚Äôs acceptable to break it into multiple lines or steps with meaningful intermediate variables. The goal is clean, easy-to-follow logic. Comments should be added for non-obvious steps or important nuances.
* **Function Documentation and Export:** All **exported functions** (and any internal ones that are complex) should have roxygen2 documentation blocks. This includes a description, parameter documentation, return value, examples, and any @seealso or @references if relevant. This not only helps users but allows tools and contributors to quickly grasp usage. Internal (non-exported) functions can have shorter comments if needed, but key ones often get a @noRd roxygen comment for clarity in the code.
* **Linting:** We use **lintr** in CI to enforce a basic style check on all packages. The default lintr configuration (based on the tidyverse style guide) is applied via a GitHub Actions workflow for each repo. Currently, we have **no custom lintr rules** beyond the defaults ‚Äì meaning the standard style conventions (indentation, object naming, whitespace, line length, etc.) are in effect. Developers should strive to write code that passes `lintr::lint_package()` cleanly. Note that our style leans toward base R and data.table usage, which is compatible with the default lintr settings.
* **Naming Conventions:** Use `snake_case` for function and variable names (consistent with base R and tidyverse conventions). S3 methods should be named `generic.class` as usual. Constants or package options can be ALL\_CAPS. Filenames in `R/` generally match the name of the primary function or concept defined in the file (to ease navigation).

By following these conventions, we ensure that code in any Artalytics repo looks familiar to those who have seen other repos. Consistent style is especially important for AI agents (like Copilot or Cursor) as it provides them a uniform context to better predict or suggest code aligned with our practices.

## CI/CD and GitHub Workflows

Continuous Integration (CI) and Continuous Deployment/Development workflows are set up for each package to maintain quality and consistency:

* **GitHub Actions Workflows:** Each repository contains YAML files under `.github/workflows/` that automate build and test processes. At minimum, there is a workflow for running `R CMD check` (which includes tests) and a workflow for linting. Many packages also have a **test coverage** workflow and possibly a deploy or pkgdown documentation workflow.

  * The **R CMD check workflow** installs the package‚Äôs dependencies (including other Artalytics packages from GitHub if listed in Remotes) and runs `R CMD check` on the package across one or more OS (commonly Ubuntu, and sometimes Windows/macOS as needed). It uses the standard r-lib/actions for consistency.
  * The **Lint workflow** runs `lintr::lint_package()` to ensure coding standards are met (without failing the build on style issues as of now).
  * The **Coverage workflow** (if present) uses **covr** to run tests and upload coverage results to a service like Codecov.
* **Quality Gates:** All workflows include steps for running **tests and linters**. A pull request must pass R CMD check (which implies all tests passed) and linting before it can be merged. This gate keeps the main branch stable and clean.
* **Secrets and Private Dependencies:** Many Artalytics packages depend on other internal packages (not on CRAN). To allow CI to install these, a GitHub Personal Access Token (PAT) is injected into the workflow environment as `GITHUB_PAT`. This token (stored in repo secrets) lets `remotes::install_github()` or similar commands access private repos. Similarly, any API keys or tokens needed for tests are provided via secrets:

  * For example, test coverage upload uses a `CODECOV_TOKEN` secret (set in the repo settings) so that `covr::codecov()` can report coverage.
  * If a package needed an API key (say for OpenAI), it would be set as an env var (e.g., `ART_OPENAI_KEY`) in the workflow, rather than hardcoded.
* **Environment Variables in CI:** Workflows set certain env vars to mimic a production-like environment for tests. For instance, `ART_USE_PG_CONF` might be set to `artprod` to indicate a production config context, and `ART_OPENAI_KEY` is set to a dummy value (`NO_VALUE`) in CI so that tests know an AI key is not available (avoiding live API calls). These environment variables ensure that tests and examples that rely on configuration or keys can run in CI safely.
* **Build Artifacts and Package Checks:** The check workflow often uploads testthat snapshot comparisons or R CMD check artifacts (like logs) for inspection if something fails. We also ensure that the `R_KEEP_PKG_SOURCE=yes` environment is set in CI so that any error reports include source references.
* **Continuous Deployment:** (If applicable) Some repos might have additional workflows, e.g., to deploy a pkgdown site or to build a Docker image for the Shiny app. Those follow the same pattern of injecting needed secrets and using standard R tools.

All CI/CD workflows are kept similar across repos, which means an agent or developer can look at the `.github/workflows/` folder and navigate them easily. If you‚Äôre in one package, you can expect the same checks and balances to apply in another, fostering a unified development workflow.

## Security and Secret Handling

Security best practices are critical in our development process. All packages follow these rules regarding secrets and sensitive information:

* **No Credentials in Repos:** **Never commit** secrets or sensitive information to the repository. This includes API keys, passwords, access tokens, and even configuration files like `.Renviron` that might contain such secrets. Git is forever; once a secret is committed, it‚Äôs compromised. So we take care to keep the codebase clean of any private credentials.
* **Environment Variables for Secrets:** Use environment variables to handle sensitive data. For example, if a function needs to access an API, it should retrieve the key via `Sys.getenv("API_KEY")` or similar. During development, you might store this in a local `.Renviron` (which is in `.gitignore`). In production or CI, these environment vars are set through secure means (never hardcoded). This way, the code remains generic and safe.
* **CI Secret Injection:** In CI workflows, any required secrets (PATs, tokens) are added via GitHub Actions secrets. As noted, the `GITHUB_PAT` for installing private packages or the Codecov token are set in the workflow YAML but their actual values come from the repo‚Äôs Settings > Secrets (and are encrypted at rest). Developers adding new CI functionality that needs a secret must add it through the GitHub interface or ask an admin, rather than putting it in code.
* **Shared Runtime Config:** The Artalytics platform uses certain **standard environment variables** at runtime across packages:

  * `ART_USE_PG_CONF` ‚Äì selects the database configuration profile (e.g., `"artprod"` for production or `"artdev"` for development).
  * `ART_RUN_AS_DEMO` ‚Äì flag (`TRUE`/`FALSE`) indicating demo mode. This can gate features or use of dummy data.
  * `ART_OPENAI_KEY` ‚Äì API key for OpenAI integration when AI features are invoked.
  * `ART_OPENSEA_KEY` ‚Äì API key for OpenSea integration.
    All packages should rely on these env vars for configuration instead of hard-coding values. For instance, if a package needs to know if it‚Äôs running in demo mode, it should check `Sys.getenv("ART_RUN_AS_DEMO")` rather than have a hardcoded switch.
* **Access Control:** Any code interfacing with external systems (databases, cloud storage, APIs) should use secure practices (e.g., use AWS SDK default credential resolution, which picks up env vars or instance roles, rather than embedding keys). Our `artcore` and related packages follow this pattern ‚Äì for example, if connecting to AWS S3, rely on the `paws.storage` library which uses credentials in env vars or config files, never in code.
* **Review for Secrets:** Code reviews include a check for accidental inclusion of secrets or personal data. We also have repository settings that scan for known secret patterns. If an API key or password is inadvertently included, rotate it and purge it immediately.

By following these practices, we maintain a secure codebase. Agents working with the repo can be assured that any needed secret will be referenced abstractly (via env var) and not visible in plaintext, and they should follow the same pattern when suggesting code (e.g., suggest `Sys.getenv("XYZ")` rather than a literal key).

## Deployment Scope

This section clarifies how each type of package fits into deployment and dependency structure within Artalytics:

* **Shiny Module Packages Deployment:** Packages like `modBrowse`, `modUpload`, `modGallery` (and future similar modules) are **not standalone applications**; they are components to be integrated into the main Artalytics Shiny app. Deployment of the platform means these module packages are installed alongside the main application, and then the main app calls their module functions to assemble the UI. As such, these packages might not run independently ‚Äì they assume the presence of the platform context (e.g., global options or `artutils` functions). When deploying, ensure the versions of these modules match the main app‚Äôs expectations. Each module package is versioned, so the main app‚Äôs DESCRIPTION will pin compatible versions.
* **Internal Utility Packages Deployment:** `artcore` and `artutils` are core internal libraries that other packages depend on. They are deployed as regular R packages (they can be installed from GitHub). They do not produce a direct user interface, but they must be installed on any system that runs the Artalytics platform (because the modules and other components call into them).
* **Dependency Boundaries:** We maintain strict layering to avoid circular or unwieldy dependencies:

  * **`artcore`** is the foundational layer. It should **not** import any other Artalytics-specific packages (no internal dependencies). It can rely on CRAN packages or third-party libraries, but it stands alone in the Artalytics ecosystem. This ensures `artcore` remains lightweight and can be used universally by the other packages (including potentially in isolation for other projects).
  * **`artutils`** is a slightly higher layer that provides additional tools, and it **may depend on `artcore` only** among internal packages. In other words, `artutils` is allowed to import `artcore` to reuse core functionality, but it should not import any module packages. This keeps `artutils` general-purpose. (External dependencies are fine, and if needed, `artutils` can suggest or import integration packages like `artopenai`, but it should primarily extend artcore).
  * **Shiny modules** (e.g., modBrowse) typically will import `artutils` (and by extension get `artcore`), but they should not import each other. They are designed to be relatively independent modules that share utilities. If modules need to communicate, that‚Äôs done via the main app logic, not by one module package importing another.
  * Other specialized packages like `artopenai` or `pixelsense` might depend on `artcore` (or `artutils`) as needed, but again, these should be leaf dependencies or shared utilities, not causing circular references.
* **Integration into Main App:** The main Artalytics application (which is a Shiny app, possibly in a separate repository) will list the module packages and core packages in its DESCRIPTION and use them. Deployment involves installing all required packages (perhaps via `renv` if the app uses it, or via GitHub remotes) on the server. The **versions are managed** such that incompatible changes are avoided (e.g., if `artcore` is updated, other packages are updated in tandem to remain compatible).
* **Example ‚Äì modGallery:** For instance, `modGallery` provides a gallery viewing module for the app. It imports `artutils` (for data access and common functions) and other needed packages like `bs4Dash` for UI components. When the app is deployed, `modGallery` must be installed and its `galleryUI()` and `galleryServer()` (hypothetical module functions) are invoked by the main app. The module itself doesn‚Äôt run on its own; it expects the app to supply things like user credentials or global reactive values. Documentation and vignettes in `modGallery` describe how it integrates (for developers), but end-users just see the combined app.
* **Isolation for Testing:** Each package can be loaded and tested in isolation (for development and CI). For example, you can run the functions of `artcore` or `artutils` by loading those packages alone. For Shiny modules, you can call their UI functions in a test app context. This modular design aids in development (each piece can be worked on independently), while the deployment brings all pieces together.

Understanding this scope and dependency structure is important for both developers and agent tools. It ensures suggestions and changes respect the boundaries (e.g., not suggesting a `modUpload` function call inside `artcore`, which would violate the dependency direction). It also clarifies that issues in one package might be resolved in another (e.g., if a module isn‚Äôt working, perhaps an `artutils` update is needed, etc.), which an agent or developer can infer by knowing these relationships.

## Package Design Best Practices

Finally, a set of best practices guide how we design each package‚Äôs content and structure beyond the mechanics already mentioned:

* **Reproducible Environments (`renv`):** Where applicable, use `renv` for managing package libraries. For standalone development of a package, this might mean using `renv` to lock dependencies (especially if a package depends on specific versions of other internal packages or non-CRAN packages). For the overall platform (main app), `renv` is likely used to pin versions of all packages including Artalytics ones. Developers should be comfortable with `renv` for creating reproducible dev environments. Commit the `renv.lock` when it‚Äôs used, and document any special steps to reproduce the environment.
* **Design for Packaging:** All code should be written with the mindset that it‚Äôs part of an R package (as opposed to one-off scripts). This means:

  * Clearly separate **functions** (in `R/` scripts) by topic, and avoid duplicating code across packages ‚Äì if two modules need the same helper, that probably belongs in `artutils` or `artcore`.
  * Use **namespaces** effectively: if a helper is only needed internally, don‚Äôt export it. If it‚Äôs useful across packages, consider exporting it (in a core package) so others can import it rather than duplicating.
  * Provide **examples** for exported functions in their documentation. This not only helps users but acts as an additional ad-hoc test and guide for expected usage.
  * Minimize side effects: functions should, as much as possible, return values rather than modify global state. This makes them more predictable and testable.
* **Testable Logic Separation:** Structure code so that business logic can be tested in isolation. For Shiny modules, this might mean separating pure functions (data processing, etc.) from reactive/server logic. For example, if `modUpload` processes a file, the file parsing function should be in `artutils` or in the module package as a normal function, which can be called in a test. The Shiny server function should mostly orchestrate calls and handle reactives, which can be simulated. Avoid burying important logic inside reactive contexts where it‚Äôs hard to call directly in a test.
* **Avoid Interactive Prompts:** No package (except possibly during an interactive demo) should use interactive prompts like `readline()` or `menu()` for inputs. Configuration should come from parameters or environment variables, not from waiting on user input at runtime. This ensures that all code can run unattended (important for servers and also for test automation). In Shiny, interactions are handled via the UI, not console prompts. In utilities, if something needs input, it should be passed as a function argument or configured via an env var or config file.
* **Consistency Across Packages:** Follow the same patterns in each package. For example, if using a certain approach to logging (say, using `futile.logger` or `log4r` in artcore), use the same across others rather than mixing logging frameworks. If `artcore` defines a function for error handling or a custom condition class, reuse that in module packages to maintain uniform error handling. This consistency means less surprise and easier maintenance.
* **Documentation of Deviations:** This AGENTS.md is the root template for standards. If a specific repository has to deviate (for good reason), document it in that repo‚Äôs AGENTS.md or README. For example, if one package must use a different style or an extra directory (maybe a `data/` directory for included datasets), make note of it in that repo‚Äôs copy of this file. That way, agent tools and contributors know the exceptions. The expectation is that such deviations are rare and justified.
* **Performance Considerations:** Be mindful of performance in design. Use vectorized operations and efficient algorithms (hence the preference for data.table). Avoid extremely large in-memory objects when possible; stream or chunk if needed (especially relevant in data processing in `artutils`). Since Shiny apps need responsive performance, any heavy computation should be optimized or moved to background (future/promises) if possible. Document any function that is potentially slow or has side effects.
* **Future-Proofing:** Where possible, code should be written to accommodate future changes. For instance, if a function interacts with an API, handle possible API changes or versioning gracefully (and document the assumptions). If a module‚Äôs interface might expand, design the UI with extension in mind (e.g., using `...` in function arguments for future options). Also, keep an eye on R deprecations ‚Äì e.g., if using a function that is soft-deprecated in R or a package, plan to transition away in the near future.

By adhering to these best practices, we ensure that each package in Artalytics is not only consistent with others but is also built in a robust, maintainable way. This makes it easier for both new developers and AI agents to understand the code and provide meaningful contributions or suggestions. The AGENTS.md serves as a living guideline ‚Äì it should be updated as new lessons are learned or tools are adopted, and those changes should propagate to all packages so the organization stays in sync.


Thanks for the clarification. I‚Äôll now regenerate AGENTS.md v2 to incorporate the new environment variable names, remove outdated references, and emphasize the requirement to use CDN-based asset retrieval via `artcore` and `artutils` wrappers instead of any local directories like `artdata`. I‚Äôll share the updated version shortly.


# Agents v2 ‚Äì Updated Configuration and Asset Handling

## Overview

This document outlines the updated configuration (version 2) for the Artalytics platform "Agents." It details critical changes to environment variables and asset management across the system‚Äôs packages. All agents (core modules and integrations such as OpenAI and OpenSea) have been refactored to use new environment variable names and to utilize a cloud CDN for asset storage instead of any local `artdata` directory. The goal of these changes is to streamline configuration and eliminate dependencies on local files, improving consistency and deployability.

## Demo Mode and Database Configuration

* **`ART_RUN_AS_DEMO`** (Boolean): **New** variable indicating if the application is running in demo mode. When set to `"TRUE"`, user actions that would normally modify data (database writes, uploading files, etc.) are **ignored and not persisted**. This allows deployment of a demo environment where anyone can explore the app as a verified user without making permanent changes. (This replaces the older `ARTDEMO` flag.) For example, the upload module (`modUpload`) checks `ART_RUN_AS_DEMO` to decide whether to actually write to the database or simply simulate the action in memory.

* **`ART_USE_PG_CONF`** (String): **New** variable telling the system which database configuration to use for connections. Typical values might be `"artdev"` or `"artprod"` to select the dev or prod database config. The core package `artcore` reads this (via `..dbc()`) to establish DB connections. This replaces the older `ARTCONF` variable. In practice, `ART_USE_PG_CONF` should match a configuration name in the `rpgconn` config file (for example, `"artprod"` for production). When the app starts, it uses this setting to connect to the correct Postgres instance.

* **`ART_PG_USER_PASSWD_DEV`** and **`ART_PG_USER_PASSWD_PRD`** (Strings): These hold the database user‚Äôs password for the development and production Postgres databases, respectively. They are referenced by the `rpgconn` library‚Äôs configuration expressions when establishing connections (used internally by `artcore::..dbc()` and `..dbd()`). For instance, if the `ART_USE_PG_CONF` is set to `"artprod"`, the `ART_PG_USER_PASSWD_PRD` value will be used for the password in the connection; if set to `"artdev"`, `ART_PG_USER_PASSWD_DEV` is used. This allows the same code to connect to different DB endpoints depending on environment, without hardcoding credentials.

**Note:** All the above environment variables should be set (e.g., via `.Renviron` or deployment environment settings) before running the application. The platform will stop with an error if required variables are missing. In the previous version, the variables `ARTCONF` and `ARTDEMO` were used for database config and demo mode. Those are now deprecated ‚Äì use `ART_USE_PG_CONF` and `ART_RUN_AS_DEMO` instead (ensure any code references are updated accordingly).

## Asset Storage via CDN (Replacing Local `artdata`)

In version 2, **no package should reference files in a local `artdata` directory**. All static and generated assets have been migrated to cloud storage (DigitalOcean Spaces) with a CDN. This is a major shift from earlier versions where a local path (specified by the old `ARTDATA` environment variable) was used to store and serve assets. Key environment variables and practices related to asset storage now include:

* **`ART_BUCKETS_KEY_ID`** and **`ART_BUCKETS_KEY_SECRET`**: These provide the Access Key ID and Secret Key for Artalytics‚Äô Spaces (object storage) buckets on DigitalOcean. They are used by the `artcore` CDN helper functions to authenticate and generate presigned URLs for private assets. With these credentials set, functions like `artcore::cdn_asset_url()` can construct URLs to assets in Spaces (for example, images or videos associated with artworks) and sign them if necessary. If these variables are not set or are empty, any attempt to access private assets will error out. Make sure to keep these keys secure and **do not commit them to code** ‚Äì they should reside in environment settings.

*Cloud Bucket Usage*: The platform uses multiple buckets (e.g., `"<PUBLIC_BUCKET>"`, `"<CERT_BUCKET>"`, `"<PRIVATE_BUCKET>"`, `"<VAULT_BUCKET>"`)

  ```r
  fs::path(<local_root>, "thumbnails", artist, paste0(artwork, ".jpeg"))
  ```

  or referenced a local path like `"artdata/private/processed/.../signature.png"`, the updated approach would be to use `cdn_asset_url("art-public", "thumbnails/artist_uuid/artwork_uuid.jpeg", signed = FALSE)` for a public thumbnail, or `cdn_asset_url("art-data", "processed/artist_uuid/artwork_uuid/signature.png")` for a private asset (which will return a presigned URL). All asset retrievals and uploads should go through such helper functions rather than direct file system calls.

> **Note**: ‚ÄúDigitalOcean Spaces‚Äù is used here as an example object-storage provider. Substitute with your own provider if different.

* **Deprecation of `ARTDATA`**: The environment variable `ARTDATA` (which pointed to a local directory for assets) is now considered deprecated. In earlier versions, the application was configured to serve this directory via Shiny‚Äôs static path (e.g., adding a resource path for `'artdata'`). In v2, **do not use `ARTDATA`**. Instead, whenever an asset is needed, retrieve its CDN URL. Similarly, any code writing files to `ARTDATA` should be refactored to upload those files to the appropriate Space (using functions provided in `artcore` or `artutils` for uploading). The codebase is in the process of replacing functions that interacted with local files (see warnings in functions like `getArtImagePath()` and `getVerificationInfo()`, which note they will be replaced by CDN logic). Going forward, all modules must treat the CDN as the single source of truth for asset storage.

## OpenAI Integration

* **`ART_OPENAI_KEY`** (String): This is the API key for OpenAI, required for any features that call OpenAI‚Äôs services (for example, generating artwork descriptions or other AI-driven content via the `artopenai` package). This replaces the older environment variable `ARTAIKEY` (which was used in version 1). The OpenAI key should be provided as a secret beginning with `sk-` (OpenAI format). Internally, the `artopenai` package uses a helper function (e.g., `..api_openai_key()`) to fetch this key from the environment. Developers do not need to call this directly; instead, when you invoke a high-level function like `artopenai::art_about_ai(img_path)`, the package will retrieve `ART_OPENAI_KEY` behind the scenes and include it in the request to OpenAI‚Äôs API. Ensure this environment variable is set in any environment where AI features are used, otherwise calls to OpenAI will fail (the package will stop if the key is missing or blank).

  *Usage Note:* The OpenAI integration in Artalytics is primarily used in pipeline contexts (e.g., analyzing an image and producing descriptive text or classifying styles). The `ART_OPENAI_KEY` should have appropriate permissions and quota for the expected usage. Always verify that this key is kept secure and not exposed in client-side code or logs.

## OpenSea Integration

* **`ART_OPENSEA_KEY`** (String): This is the API key for OpenSea, used by the `artopensea` package to interact with the OpenSea API. It replaces the older variable `ARTOSKEY` from the previous version. The OpenSea integration is used in the app to fetch NFT data, collections, and other marketplace information as needed. Similar to OpenAI, the `artopensea` package defines an internal helper `..api_opensea_key()` that reads this environment variable and is utilized whenever an OpenSea API call is made (for example, in functions like `get_os_account()` and others).

  When using any Artalytics features that query OpenSea (such as linking a crypto wallet or displaying NFTs), ensure `ART_OPENSEA_KEY` is set to a valid API key obtained from OpenSea‚Äôs developer settings. If it‚Äôs not set, the OpenSea requests will not include the required API key header and will likely be rejected by the OpenSea API. As with other secrets, do not hardcode this key; store it in the environment configuration.

## Summary of Changes from Previous Version

In summary, version 2 of the Agents configuration standardizes and secures the platform setup:

* **Environment Variable Renames:** The configuration now uses clearer, more descriptive environment variable names. For example, `ARTDEMO` ‚Üí `ART_RUN_AS_DEMO`, `ARTCONF` ‚Üí `ART_USE_PG_CONF`, `ARTAIKEY` ‚Üí `ART_OPENAI_KEY`, `ARTOSKEY` ‚Üí `ART_OPENSEA_KEY`. All code and deployment scripts should be updated to use the new names. The older names are considered **outdated** and support for them will be removed in the codebase. An example of the old variables in a Docker environment file is shown here for reference: . These should all be replaced with their new counterparts as described above.

* **Elimination of Local Asset Directory:** No agent or module should directly read from or write to local paths under an `artdata` directory. Instead, use the CDN approach (DigitalOcean Spaces). This centralizes asset management and allows scalable access to media files. Functions that previously returned local file paths now need to return URLs or data streams from the CDN. This change improves the reliability of the app in distributed environments (e.g., Docker containers or cloud deployments) since it no longer relies on a shared filesystem for assets.

By adopting these changes, all "Agents" in the Artalytics platform will operate under the updated configuration, ensuring a smoother development and deployment process. **Developers maintaining or extending the platform should double-check that**:

1. **All references to old environment variables are updated** (and the new variables are set in the runtime environment).
2. **All asset handling code uses the provided CDN utility functions** rather than file system calls or hard-coded paths.
3. **API keys and credentials are accessed through the intended wrappers or config** (which is typically already the case in the provided packages, as they abstract the environment access for you).

Following these guidelines will keep the platform consistent with version 2 standards and avoid issues related to missing configuration or inaccessible assets.

-----

NOTE: If folder .agents is present, review files in its' folder "Research" first followed by files in its' folder "Tasks".

---

## üîí Scoped Mocking & Test Isolation in **testthat** (Guidelines for All Repositories)

Unit‚Äëtests should be **self‚Äëcontained** and **order‚Äëindependent**.
The most common source of hidden coupling is a mock or global object that remains active after the test that created it. Follow the rules below whenever you add or modify tests:

| **Rule**                                                                                               | **Why**                                                                                                                                                 | **How**                                                                                                                               |
| ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| 1. **Mock inside a `test_that()` (or `with_mocked_bindings()` block), never at file‚Äëtop level.**       | Top‚Äëlevel mocks persist for the lifetime of the test file and bleed into any tests executed later, sometimes in entirely different files.               | `r test_that("handles upload", { local_mocked_bindings( write_art_data = function(...) TRUE, .package = "mypkg" ) # ‚Ä¶assertions‚Ä¶ }) ` |
| 2. **Use `local_*()` helpers (`local_mocked_bindings()`, `local_tempdir()`, `local_envvar()`, etc.).** | All `local_*()` helpers register automatic teardown code via **withr**, guaranteeing cleanup even if the test fails.                                    | `r test_that("creates plot", { tmp <- withr::local_tempdir() # deleted automatically on exit ‚Ä¶ }) `                                   |
| 3. **Keep shared state inside the block.**                                                             | Tests must be runnable in any order and in parallel. Shared objects (e.g., `uploads <- list()`) defined outside the block create ordering dependencies. | `r test_that("collects uploads", { uploads <- list() ‚Ä¶expect_length(uploads, 3) }) `                                                  |
| 4. **Do *not* call `withr::deferred_clear()` unless you also replace the corresponding cleanup.**      | Clearing the teardown registry disables automatic restoration of mocks and temp files.                                                                  |                                                                                                                                       |
| 5. **Compare values, not classes, unless class identity is critical.**                                 | Helpers like `fs::path_rel()` return an `fs_path` object. Use `expect_equal()` (or `as.character()`) unless you explicitly need `expect_identical()`.   | `r expect_equal(fs::path_rel(p, fs::path_temp()), expected_prefix) `                                                                  |

### Quick Example

```r
test_that("downloader uploads PNGs", {
  # isolate filesystem
  tmp_dir <- withr::local_tempdir()

  # capture side‚Äëeffects
  calls <- list()

  # scoped mocks
  local_mocked_bindings(
    write_art_data = function(path, ...) { calls$path <<- path; TRUE },
    .package = "mypkg"
  )
  local_mocked_bindings(
    ggsave = function(filename, ...) { file.create(filename); invisible(filename) },
    .package = "ggplot2"
  )

  # run code under test
  result <- create_plots("id123", out_dir = tmp_dir)

  # assertions
  expect_true(result)
  expect_true(fs::file_exists(fs::path(tmp_dir, "1.png")))
  expect_identical(calls$path, tmp_dir)
})
```

### Key Take‚Äëaways

* **Always clean up after yourself**‚Äîbut let `local_*()` helpers do the work.
* **One test = one sandbox**: everything the test needs should be created inside the block and disposed of automatically.
* **No hidden dependencies**: if a test needs a mock or a fixture, declare it explicitly in that test.

Adhering to these practices keeps the test suite reliable, parallel‚Äësafe, and easier for future contributors to extend.

---

